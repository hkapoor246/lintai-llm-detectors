###############################################
#  Lintai – example .env for LLM providers    #
###############################################

# -------------------------------------------------
# 1) Select provider (openai | azure | anthropic | gemini | cohere | dummy)
# -------------------------------------------------
LINTAI_LLM_PROVIDER=openai

# -------------------------------------------------
# 2) OpenAI Cloud (api.openai.com)
# -------------------------------------------------
# NOTE: Make sure the dash after “sk” is a normal ASCII hyphen (-)
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Optional: custom base URL (proxy, staging, etc.)
# OPENAI_API_BASE=https://api.openai.com/v1/

# -------------------------------------------------
# 3) Azure OpenAI  (Uncomment these and set provider=azure)
# -------------------------------------------------
# LINTAI_LLM_PROVIDER=azure
# AZURE_OPENAI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# AZURE_OPENAI_ENDPOINT=https://my-resource.openai.azure.com/
# AZURE_OPENAI_API_VERSION=2025-01-01-preview
#
# The deployment *name* goes in OPENAI_MODEL below.

# -------------------------------------------------
# 4) Shared option – deployment / model name
# -------------------------------------------------
OPENAI_MODEL=gpt-4o-mini     # OpenAI model or Azure deployment name

# -------------------------------------------------
# 5) Anthropic Claude
# -------------------------------------------------
# ANTHROPIC_API_KEY=sk-antropic-xxxxx

# -------------------------------------------------
# 6) Google Gemini / PaLM2
# -------------------------------------------------
# GOOGLE_API_KEY=AIzaSyXXXXX

# -------------------------------------------------
# 7) Cohere Command‑R
# -------------------------------------------------
# COHERE_API_KEY=_cstxxxxx

# -------------------------------------------------
# 8) Local Ollama (future provider)
# -------------------------------------------------
# OLLAMA_BASE_URL=http://localhost:11434
